\gdef \the@ipfilectr {@-1}
\contentsline {figure}{\numberline {1.1\relax }{\ignorespaces 64-bit string for floating point expression. The last bit is used for the sign and 11 bits from $b_{52}$ to $b_{62}$ express the exponent. The remaining 52 bits express the mantissa.\relax }}{5}{figure.caption.10}%
\contentsline {figure}{\numberline {1.2\relax }{\ignorespaces Discreteness of floating point numbers. $\epsilon $ is the machine epsilon discussed in Sec. \ref {sec:epsilon}.\relax }}{5}{figure.caption.11}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-2}
\contentsline {figure}{\numberline {2.1\relax }{\ignorespaces Illustration of various numerical derivatives. The exact derivative is the slope of the curve at $x$, which is shown as the dotted line. The forward finite difference method shown in green underestimates the slop whereas the backward finite difference method shwon in blue overestimates it. The mean finite different method shown in red looks very close to the exact derivative.\relax }}{24}{figure.caption.17}%
\contentsline {figure}{\numberline {2.2\relax }{\ignorespaces Output of Example \ref {ex:derivatives}. The left panel shows numerical derivatives for wide ranges of $h$. As $h$ decreases from $h=1$ to $h=0.01$, the derivative converges to 1 (at least in our eyes). As $h$ further decreases, the values of all methods remain the same until $h \approx \epsilon _\text {m}$. Below it, the derivative abruptly goes to 0. The numerical method fails due to round-off error. To see more details, the right panel plots the error. As $h$ decreases, the error of the forward and backward finite difference methods decreases in the same way as $h$ until $h \approx 10^{-8}$ but the error increases when $h$ is further reduced. The mean finite difference method shows smaller error than the two other methods and the error decreases as $h^2$ up to $h \sim 10^{-5}$. The best result is given by the mean finite difference method with $h \approx 10^{-5}$.\relax }}{26}{figure.caption.18}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-3}
\contentsline {figure}{\numberline {3.1\relax }{\ignorespaces Illustration of simple numerical integration methods\relax }}{36}{figure.caption.23}%
\contentsline {figure}{\numberline {3.2\relax }{\ignorespaces Output of Example \ref {ex:integrals}.\relax }}{40}{figure.caption.24}%
\contentsline {figure}{\numberline {3.3\relax }{\ignorespaces Due to the divergence at $x=0$, it is difficult integrate the original function (black line). The blue line has the same singularity at $x=0$ but can be analytically integrate. The difference (red line) does not have a singularity and hence common numerical integration works fine.\relax }}{42}{figure.caption.25}%
\contentsline {figure}{\numberline {3.4\relax }{\ignorespaces Classical Oscillation\relax }}{45}{figure.caption.26}%
\contentsline {figure}{\numberline {3.5\relax }{\ignorespaces Geometry of scattering in relative coordinate.\relax }}{46}{figure.caption.27}%
\contentsline {figure}{\numberline {3.6\relax }{\ignorespaces The left panel shows the original integrand. The green area need to be numerically integrated. The right panel shows the integrand after the singularity is removed. The blue area need to be integrated. Note the difference in scale between two plots. The blue area is much smaller than the green area. Parameter values $k=a=E=1$ are used.\relax }}{46}{figure.caption.28}%
\contentsline {figure}{\numberline {3.7\relax }{\ignorespaces Correction term, the integral in Eq. (\ref {eq:delta_heat_electron}).\relax }}{49}{figure.caption.29}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-4}
\contentsline {figure}{\numberline {4.1\relax }{\ignorespaces A root of the quadratic equation $\epsilon x^2+x+1/4=0$ is evaluated with two different methods with $\epsilon <1$. The left hand side of Eq. (\ref {eq:quad_trick2}) is initially approaching to the correct limit as $\epsilon $ decreases. However, it goes erroneous below $\epsilon =10^{-5}$. On the other hand, the right hand side steadily converges to the right answer.\relax }}{64}{figure.caption.39}%
\contentsline {figure}{\numberline {4.2\relax }{\ignorespaces Bisection method. Starting with initial bracket $(x_0, x_1)$, the bracket is at each iteration halved to $(x_2,x_1)$, $(x_2,x_3)$, $(x_2, x_4)$, $(x_5, x_4)$, $\cdots $.\relax }}{67}{figure.caption.40}%
\contentsline {figure}{\numberline {4.3\relax }{\ignorespaces Newton-Raphson method. Starting with initial guess $x_0$, the line tangent to the curve at the current point $x_n$ is used to find a new imroved root $x_{n+1}$. If the initial guess $x_0$ is close enough to the true root, this procedure rapidly converges to it.\relax }}{67}{figure.caption.40}%
\contentsline {figure}{\numberline {4.4\relax }{\ignorespaces The function used in Example \ref {ex:cos3x_sinx}. The smallest positive root is bracketed between 0.2 and 0.8 (between the dashed lines) by visual inspection.\relax }}{70}{figure.caption.41}%
\contentsline {figure}{\numberline {4.5\relax }{\ignorespaces Ferromagnetic Phase Transition\relax }}{72}{figure.caption.42}%
\contentsline {figure}{\numberline {4.6\relax }{\ignorespaces Quantum particle in a finite suare well\relax }}{73}{figure.caption.43}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-5}
\contentsline {figure}{\numberline {5.1\relax }{\ignorespaces Illustration of the Euler method\relax }}{93}{figure.caption.47}%
\contentsline {figure}{\numberline {5.2\relax }{\ignorespaces Illustration of the Predictor-Corrector Method\relax }}{95}{figure.caption.48}%
\contentsline {figure}{\numberline {5.3\relax }{\ignorespaces Output of Example \ref {ex:free_falling1}. The left panel shows the velocity as a function of time. All three lines look identical. The right panel shows the absolute errors. The error in the predictor-corrector method is clearly square of the error in the Euler method.\relax }}{96}{figure.caption.49}%
\contentsline {figure}{\numberline {5.4\relax }{\ignorespaces Illustration of the second order Runge-Kutta Method.\relax }}{96}{figure.caption.50}%
\contentsline {figure}{\numberline {5.5\relax }{\ignorespaces Output of Example \ref {ex:free_falling2}. The left panel shows the velocity as a function of time. All three lines look identical. The right panel shows the absolute errors. The 4th order Runge-Kutta method is clearly more accurate than the 2nd order method.\relax }}{99}{figure.caption.51}%
\contentsline {figure}{\numberline {5.6\relax }{\ignorespaces Output of Example \ref {ex:free_falling3}. The left panel shows the velocity as a function of time. The circles on the top indicates the time step. The right panel shows the absolute errors which remains below the tolerance $10^{-3}$.\relax }}{100}{figure.caption.52}%
\contentsline {figure}{\numberline {5.7\relax }{\ignorespaces Output of Example \ref {ex:twocars}. Left: The velocity of each car. At the end two cars travel at the same velocity. Right: The difference in velocities. The velocity difference decreases exponentially. The 2nd order Runge-Kutta method with $h=0.02$ is used.\relax }}{102}{figure.caption.53}%
\contentsline {figure}{\numberline {5.8\relax }{\ignorespaces Left: Trajectory of a simple harmonic oscillator ($\omega =1$): The Newtons equation of motion is integrated with 4th order Runge-Kutta method ($h=0.05$). Right: Absolute error. The error is very small but gradually increasing as the number of iterations increase. \relax }}{103}{figure.caption.54}%
\contentsline {figure}{\numberline {5.9\relax }{\ignorespaces Left: Trajectory of a simple harmonic oscillator ($\omega =1$): The Newtons equation of motion is integrated with Verlet method ($h=0.05$). Right: Absolute error. The error is small but considerably larger that of 4th-order Runge-Kutta method in Fig. \ref {fig:harmonic_oscillator1}. \relax }}{104}{figure.caption.55}%
\contentsline {figure}{\numberline {5.10\relax }{\ignorespaces Limit cycle in the Brusselator dynamics. Parameter values: $a=1$ and $b=2.3$\relax }}{105}{figure.caption.56}%
\contentsline {figure}{\numberline {5.11\relax }{\ignorespaces Left: Erroneous oscillation in the magnitude of electric field. Right: Three--dimensional phase plot of $E$, $P$ and $D$ showing a strange attractor. Parameter values: Type C in Table \ref {tbl:maxwell_bloch} and $\lambda =23$\relax }}{107}{figure.caption.58}%
\contentsline {figure}{\numberline {5.12\relax }{\ignorespaces Left: The trajectory of the oscillators. Each oscillator has its own natural frequency $\omega _1=1.0$ and $\omega _2=1.2$. Initially the two oscillators are out of phase. Despite of these differences, they are quickly synchronized and oscillate at the same frequency. Left: the phase difference rapidly changes at the beginning but settles to a constant phase difference. [The 2nd-order Runge-Kutta is used with $h=0.01$.]\relax }}{108}{figure.caption.59}%
\contentsline {figure}{\numberline {5.13\relax }{\ignorespaces The numerical instability with the Euler method. Left: Time evolution of angular coordinate $\theta $. The result of the Verlet method oscillates periodically as expected. However, the output of the Euler method oscillates with increasing amplitude and diverges at the end. Right: Mechanical energy. The energy with the Verlet method conserves but that of the Euler method keeps increasing. Integration step size $h=0.01$ is used.\relax }}{109}{figure.caption.60}%
\contentsline {figure}{\numberline {5.14\relax }{\ignorespaces Scattering by a screened Coulomb force. Left: trajectories with different impact parameters. Notice the shadow cone behind the target where the particle cannot enter. Right: Scattering angle $\theta $ determined by the simulation.\relax }}{111}{figure.caption.61}%
\contentsline {figure}{\numberline {5.15\relax }{\ignorespaces Chaotic motion of a double pendulum. Left: Two angular coordinates are randomly drifting. Right: The trajectory of the bottom bob shows chaotic motion. Parameter values: $m_1=2\, kg$, $m_2=1\, kg$, $L_1=1\, m$, $L_2=2\, m$, $h=0.02$.]\relax }}{112}{figure.caption.62}%
\contentsline {figure}{\numberline {5.16\relax }{\ignorespaces A spring pendulum for Problem \ref {ch:ode1}.3.\relax }}{113}{figure.caption.63}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-6}
\contentsline {figure}{\numberline {6.1\relax }{\ignorespaces The output of Example \ref {ex:shoot_rocket}. Improvement of the solution as the secant method is iterated. Initial guesses (step 1 and 2) are far from the correct answer but the iteration quickly converges to the right answer.\relax }}{143}{figure.caption.67}%
\contentsline {figure}{\numberline {6.2\relax }{\ignorespaces The output of Example \ref {ex:poisson1d}. Left: The profile of the charge density (black), the numerical potential (red) and exact solution (blue). Right: The boundary value of the derivative is iteratively optimized to the correct boundary condition.\relax }}{145}{figure.caption.68}%
\contentsline {figure}{\numberline {6.3\relax }{\ignorespaces The numerical solution (red) to Eq. (\ref {eq:airy}) is compared with the airy function (blue) provided by MATLAB. Two curves are normalized at $x=0$.\relax }}{147}{figure.caption.69}%
\contentsline {figure}{\numberline {6.4\relax }{\ignorespaces Left: The numerical solution to Eq. (\ref {eq:heating_rod}). Right: Error after each secant iteration.\relax }}{149}{figure.caption.70}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-7}
\contentsline {figure}{\numberline {7.1\relax }{\ignorespaces Illustration of the shooting method. Equation (\ref {eq:eigen_modes}) is integrated with three different values of $\lambda $. When $\lambda =-9$ (blue) , the solution overshoots the target boundary. On the other hand, when $\lambda = -11$ (turquoise), the solution undershoots it. Therefore, the correct answer should be between the two values. When $\lambda =-9.8696044$ (red), the solution hits the target and thus it is the eigenvalue.\relax }}{170}{figure.caption.74}%
\contentsline {figure}{\numberline {7.2\relax }{\ignorespaces Wave functions of quantum harmonic oscillator. Left: Symmetric state. When the value of $\lambda $ is not right (blue and turquoise), the wave function is not smoothly connected at $x=0$. When $\lambda $ is the correct eigenvalue, the curve is smooth everywhere. Right: Anti-symmetric state. Similarly to the symmetric function, when the value of $\lambda $ is not right (blue and turquoise), the wave function jumps at $x=0$.\relax }}{171}{figure.caption.75}%
\contentsline {figure}{\numberline {7.3\relax }{\ignorespaces Wavefuntions and eigenvalues of a quantum bouncing particle. Left: The energy eigenfunctions of the lowest three states. Right: The levels of the three lowest energy eigenstates.\relax }}{172}{figure.caption.77}%
\contentsline {figure}{\numberline {7.4\relax }{\ignorespaces Left: The Morse potential and the three lowest eigenvalues. Right: Wavefuntions corresponding toe the three eigenvalues shown in the left panel.\relax }}{174}{figure.caption.79}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-8}
\contentsline {figure}{\numberline {8.1\relax }{\ignorespaces After two steps of forward elimination, 3-by-3 submatrix remains non-triangular. To find the next pivot, find the maximum of $A_{33}/S_3$, $A_{43}/S_4$, and $A_{53}/S_5$. The row carrying the maximum goes to the top of the submatrix.\relax }}{202}{figure.caption.83}%
\contentsline {figure}{\numberline {8.3\relax }{\ignorespaces A small example of tree graph. It has 10 vertices and 9 edges.However, there is no loop.\relax }}{214}{figure.caption.85}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-9}
\contentsline {figure}{\numberline {9.1\relax }{\ignorespaces Diagram of 2D Newton-Raphson step. \textit {Left}: The landscape of $f_1(\vb {x})$. The thick line indicates the nullcline $f_1(\vb {x})=0$. Starting at the initial guess (circle), $-\grad f_1(\vb {x})$ (arrow) tells the steepest descent toward the nullcline. \textit {Center}: The landscape for $f_2(\vb {x})$. Similar to the left panel, the arrow point to the nullcline $f_2(\vb {x})=0$. \textit {Right}: The superimpose of the left and center panel. The crossing points of two nullclines are the solutions. The vector sum of two steepest descent direction (black arrow) approximately points the solution.\relax }}{240}{figure.caption.89}%
\contentsline {figure}{\numberline {9.2\relax }{\ignorespaces Convergence of Example \ref {ex:newton_raphson_2d}. Starting at $x=1$ and $y=0$, the Newton-Raphson procedure gradually improves the output toward the root of nonlinear equation (\ref {eq:nonlinear_2d}). The step factor $\alpha =0.1$ is used in this case.\relax }}{243}{figure.caption.90}%
\contentsline {figure}{\numberline {9.3\relax }{\ignorespaces Fixed points of the Maxwell-Bloch equations for typa A laser. (See Section 4.3.2 for parameter values.) After several iteration, the Newton-Raphson method converges to the solution.\relax }}{244}{figure.caption.91}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-10}
\contentsline {figure}{\numberline {10.1\relax }{\ignorespaces Two different boundary conditions for the chain of atoms.\relax }}{266}{figure.caption.95}%
\contentsline {figure}{\numberline {10.2\relax }{\ignorespaces Tight binfing model of atomic chains with two different boundary conditions. The wavefunctions corresponding to the lowest three energy are plotted.\relax }}{267}{figure.caption.96}%
\contentsline {figure}{\numberline {10.3\relax }{\ignorespaces A chain of atoms with an impurity at $K=3$. The wavefunctions of lowest three energy states are plotted. Note that electrons in the lowest two energy states do not hop to atom 1 and 2. The impurity seems blocking it.\relax }}{268}{figure.caption.97}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-11}
\contentsline {figure}{\numberline {11.1\relax }{\ignorespaces Bit reversed order for $N=8$.\relax }}{293}{figure.caption.101}%
\contentsline {figure}{\numberline {11.2\relax }{\ignorespaces Fast Fourier transform of a gaussian function. \textit {Top left}: The original function value centered around $t=0$. The dashed lines indicate the lower and upper bounds at $\pm T/2$. \textit {Bottom left}: The lower half ($t<0$) of the function is shifted by $T$. Now the bounds are $(0,T)$ indicaed by the dashed line. \textit {Top right}: Fourier transform of the Gaussian generated by MATLAB function \texttt {ifft()}. The lower and upper bounds of the frequency is 0, $\Omega =2\pi N/T$ indicated by the dashed line. \textit {Bottom right}: The upper half of the data is shifted by $-\Omega $. Now, the Fourier transform is peaked around $\omega =0$.\relax }}{295}{figure.caption.102}%
\contentsline {figure}{\numberline {11.3\relax }{\ignorespaces Fourier transform of the Gaussian distribution, which is again Gaussian in the Fourier space. The output of FFT agrees well with the exact solution.\relax }}{296}{figure.caption.103}%
\contentsline {figure}{\numberline {11.4\relax }{\ignorespaces Laplacian by FFT: Output of Example \ref {ex:fft_gaussian}.\relax }}{297}{figure.caption.104}%
\contentsline {figure}{\numberline {11.5\relax }{\ignorespaces Normal modes of coupled oscillators by spectral analysis. \textit {Left}: Trajectories of each oscillators. It looks quite random. However, they are just a combination of three periodic motion. \textit {Right} Power spectrum of the trajectory. Three peaks corresponding to eigenfrequencies are sharp and clear. The dashed lines indicates the eigenfrequencies obtained by eigenvalue anaysis in Sec. 9.3.1. They all match to the peak positions.\relax }}{299}{figure.caption.105}%
\contentsline {figure}{\numberline {11.6\relax }{\ignorespaces The probability density in momentum space for a quantum harmonic oscillator\relax }}{300}{figure.caption.106}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-12}
\contentsline {figure}{\numberline {12.1\relax }{\ignorespaces Linear and cubic spline of the data given in Exampel \ref {ex:spline_linear}. The dashed curve is the original function from which the data set was generated.\relax }}{316}{figure.caption.111}%
\contentsline {figure}{\numberline {12.2\relax }{\ignorespaces Polynomial Fitting of random data given in Table \ref {tbl:spline_data}. The open circles show the original data and the line plots the interpolation by the polynomial obtained by the Vandermonde matrix.\relax }}{317}{figure.caption.112}%
\contentsline {figure}{\numberline {12.3\relax }{\ignorespaces Linear regression: The solid line is obtained by the linear regression formula (\ref {eq:linear_regression}) with $\sigma _i=1$. Despite that the data is noisy, the fitted line represents the data set very well.\relax }}{321}{figure.caption.113}%
\contentsline {figure}{\numberline {12.4\relax }{\ignorespaces Least square fitting of the data set in Table \ref {tbl:quadratic_regression} with a quadratic function. THe error bar is large where the data is close to zero. The $\chi ^2$ function allows those points to stay off the curve but not too far.\relax }}{322}{figure.caption.116}%
\contentsline {figure}{\numberline {12.5\relax }{\ignorespaces The least square fitting of the reaction rate. (a) The fitting is done with variables $\log k$ and $\beta $ sicne the theory perdict a straight line with those variables. The fitted line (solid line) matches well to the data set (open circle). (b) The fitted curve is shown in the original variable $k$ and $T$. The cureve is no longer a straight line but represent the data set quite well.\relax }}{325}{figure.caption.118}%
\contentsline {figure}{\numberline {12.6\relax }{\ignorespaces Nonlinear least square fitting of the noisy data set in Table \ref {tbl:Lorentzian} with a Lorentzian function. \textit {Left}; Depite of the error bars, the Gauss-Newton method managed to fit the data to the desired function. \textit {Right}: The $\chi ^2$ decreases as the iteration proceeds.\relax }}{326}{figure.caption.120}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-13}
\contentsline {figure}{\numberline {13.1\relax }{\ignorespaces Three different types of boundary conditions for diffusion equations. (a) The particle is reflected by the wall [Neumann boundary]. (b) The particle is perfectly absorbed on the wall [Dirichlet boundary]. (c) Some particles are reflected and others absorbed on the wall with a transition rate $k_\text {on}$. The particles on the wall can desorb with a transition rate $k_\text {off}$. This situation can be dealt with the Robin boundary condition.\relax }}{349}{figure.caption.126}%
\contentsline {figure}{\numberline {13.2\relax }{\ignorespaces A solution to the diffusion equation with the Neumann boundary at $x=\pm 10$. The left panel shows the time-evolution of the density at from $t=10$ to $t=100$, starting with an initial condition, $\rho (x,0)=\delta (x)$. The right panel shows the density at $t=20$, which is in good agreement with the exact solution.\relax }}{352}{figure.caption.127}%
\contentsline {figure}{\numberline {13.3\relax }{\ignorespaces Quantum tunneling through the square potential barrier. The left panel shows the probability density of the initial wave packet moving toward the potential barrier. The right panel shows the probability density after the collision with the potential barrier. A broad peak in the right side of the potential barrier indicates that the fraction of the packet tunnels through the barrier.\relax }}{356}{figure.caption.128}%
\contentsline {figure}{\numberline {13.4\relax }{\ignorespaces Time evolution of pattern formation. Initially, the chemicals are randomly distributed. As time goes, a pattern begins to appear. By $t=100$, a two dimensional crystal like structure is formed. However, the pattern does not have a precise periodicity or symmetry yet. At $t=2000$, the system reaches a steady state. The spot size is now identical and they form a hexagonal close-packing structure. Parameter values are $a=2.5$, $b=5.0$, $D_u=0.2$, and $D_w=1.6$. Periodic boundary condition with $L=20$ is used. The discretization parameters are $h=1$, and $\Delta t=0.125\times 10^{-2}$.\relax }}{358}{figure.caption.129}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-14}
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-15}
\contentsline {figure}{\numberline {15.1\relax }{\ignorespaces Virtual Die\relax }}{369}{figure.caption.132}%
\contentsline {figure}{\numberline {15.2\relax }{\ignorespaces Monte Carlo method to evaluate the area inside a circle. The area inside the circle equals $4 N_\text {red}/(N_\text {blue}+N_\text {red})$ where the factor 4 is the area of the square.\relax }}{370}{figure.caption.133}%
\contentsline {figure}{\numberline {15.3\relax }{\ignorespaces Monte Carlo evaluation of the volume of hypersphere. As the number of sampling points increases, the result approaches to the exact value.\relax }}{371}{figure.caption.134}%
\contentsline {figure}{\numberline {15.4\relax }{\ignorespaces Mapping from random number $x$ uniformly distributed between 0 and 1 to $y$ exponentially distributed from 0 to $\infty $. The transformation function is $y=-\ln (x)$.\relax }}{372}{figure.caption.135}%
\contentsline {figure}{\numberline {15.5\relax }{\ignorespaces Two generators of normally distributed random generator. The distribution is constructed from 100,000 realizations. The distribution of $S_{12}$ is strictly zero for $|x|>6$ and thus rare events are not included. In principle, the Box-Muller method can generate rare random numbers. However, it is so rare that $|x|>6$ is not realized with this 100,000 sampling.\relax }}{373}{figure.caption.136}%
\contentsline {figure}{\numberline {15.6\relax }{\ignorespaces Generating histogram from continuous random numbers. Circles indicate the random numbers. The number of the circles in a bin corresponds to the height of the bar above it.\relax }}{374}{figure.caption.137}%
\contentsline {figure}{\numberline {15.7\relax }{\ignorespaces Snapshot of the sedimentation diffusion equilibrium\relax }}{376}{figure.caption.138}%
\contentsline {figure}{\numberline {15.8\relax }{\ignorespaces A random deposition model with surface relaxation. The lateral position is randomly selected and a particle is placed on the surface particle from the above. Then, it steps down to the local minimum.\relax }}{377}{figure.caption.139}%
\contentsline {figure}{\numberline {15.9\relax }{\ignorespaces Surface growth with the ballistic deposition model without surface relaxation.\relax }}{378}{figure.caption.140}%
\contentsline {figure}{\numberline {15.10\relax }{\ignorespaces Surface growth with the ballistic deposition model with surface relaxation.\relax }}{379}{figure.caption.141}%
\contentsline {figure}{\numberline {15.11\relax }{\ignorespaces Growth of a surface based on a ballistic deposition model with possibility of overhang structures.\relax }}{380}{figure.caption.142}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-16}
\contentsline {figure}{\numberline {16.1\relax }{\ignorespaces One-dimensional discrete random walk. The blue arrows indicate a realization of 6-steps trajectory, $RRLRRL$\relax }}{400}{figure.caption.146}%
\contentsline {figure}{\numberline {16.2\relax }{\ignorespaces Monte Carlo simulation of discrete random walk. 100000 trajectories are used to get the statistics. Left: The solid and dashed red lines show the mean trajectory and the deviation from the mean. Thin solid lines are individual trajectories. Right: The distribution at step $N=1000$. It fits exactly to the Gaussian distribution (red line) with variance $\sigma ^2=N$.\relax }}{401}{figure.caption.147}%
\contentsline {figure}{\numberline {16.3\relax }{\ignorespaces Simulation of persistent random walk. 100000 trajectories are sampled with a persistent jump probability $p=0.75$. The left panel shows that the mean position remains zero but the variance grows faster than that of the normal random walk. The right panel shows the distribution of the particles. The red line indicates the distribution of the normal random walk (Gaussian).\relax }}{402}{figure.caption.148}%
\contentsline {figure}{\numberline {16.4\relax }{\ignorespaces Simulation of two-dimensional discrete random walk. Statistics is taken over 100000 trajectories.\relax }}{403}{figure.caption.149}%
\contentsline {figure}{\numberline {16.5\relax }{\ignorespaces Two deposition models: In the ballistic deposition model, the particles do not diffuse. The lateral position is randomly selected and stick to the first particle in a cluster. In the diffusion limited model, on the other hand, the particles diffuse laterally as well as vertically. They stick to the first particle they hit. Due to the random walk, they can attached to the cluster at any location.\relax }}{404}{figure.caption.150}%
\contentsline {figure}{\numberline {16.6\relax }{\ignorespaces Diffusion limited aggregates (DLA)\relax }}{405}{figure.caption.151}%
\contentsline {figure}{\numberline {16.7\relax }{\ignorespaces Dendrite Crystals\relax }}{406}{figure.caption.152}%
\contentsline {figure}{\numberline {16.8\relax }{\ignorespaces Simulation of Parrondo Game. 50000 people played Games in a Casino. When they play only Game A, on average people lose their money. Similarly, only Game B is played, again on average people lose their money. Now they play Game A for several times and switch to Game B. After playing Game B for several times switch back to Game A. Then, repeat this many times. You always win!\relax }}{407}{figure.caption.153}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-17}
\contentsline {figure}{\numberline {17.1\relax }{\ignorespaces Velocity distribution generated by the Metoropolis method. The red line plots the Maxwell distribution.\relax }}{428}{figure.caption.157}%
\contentsline {figure}{\numberline {17.2\relax }{\ignorespaces Examples of coupling energy. \textit {Left}: Each f the four pairs has energy $-J$ and thus the total energy is $-4J$. \textit {Right}: Two pairs have energy $-J$ each and the other two pairs have $+J$ each. Therefore, the total energy is zero.\relax }}{430}{figure.caption.158}%
\contentsline {figure}{\numberline {17.3\relax }{\ignorespaces Snapshot of the microscopic states. Blue sites indicate spin-up and yellow spin-down. When temperature is well below the critical temperature (a), one color dominates. This sample happens to be dominated by yeallow but states dominated by blue also happens with the equal probability. Well above the critical temperature (c), blue and yellow are scatted evenly. Although a large clusters are sill seen, they should disappear as temperature goes up further. Near the critical temperature (b), blue and yellow are equally likely but each color forms a large cluster. along with many smaller clusters. \relax }}{431}{figure.caption.159}%
\contentsline {figure}{\numberline {17.4\relax }{\ignorespaces Sampling of magnetization. The horizontal axis indicates the individual sample. When temperature is well below the critical temperature (a), all sampled state have similar large negative magnetization. The flusctuation is rather small. Well above the critical temperature (c), all sampled state have small magnetization close to zero. The fluctuation is bigger than that of (a) due to higher temperature. Near the critical temperature (b), each sample has quite different value of the magnetization. The fluctuation of (b) is even larger than that of the higher temperature state (c). \relax }}{431}{figure.caption.160}%
\contentsline {figure}{\numberline {17.5\relax }{\ignorespaces Monte Carlo Simulation of Ising model. The dashed line indicates the theoretical prediction of the critical temperature. The top panel shows the spontaneous magnetization below a critical temperature around $T=2.4$. The heat capacity has a sharp peak at the critical temperature as shown in the middle panel. On the other hand, the energy plotted in the bottom panel does not show any dramatic change across the transition points.\relax }}{432}{figure.caption.161}%
\contentsline {figure}{\numberline {17.6\relax }{\ignorespaces Realization of clusters on the $32\times 32$ lattice. No percolation is observed for $p=0.50$. Increasing the probability to $p=0.58$, a large cluster (red) shows percolation in the horizontal direction.\relax }}{433}{figure.caption.162}%
\contentsline {figure}{\numberline {17.7\relax }{\ignorespaces Hoshen-Kopelman cluster labeling scheme. (a) Inspect each site from the bottom left corner along each column. Supposed that all sites upto the red one is already inspected and a label is assigned to each cluster. In this example, there are four clusters labeled 1 through 4. Now we inspect if the red site is a part of the previously known cluster. There are four five possibilities shown in (b) -- (f).\relax }}{434}{figure.caption.163}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-18}
\contentsline {figure}{\numberline {18.1\relax }{\ignorespaces A heavy particle experiences many collisions with smaller particles but its velocity remains the same for a certain short period of time. The force exerted on the particle is the sum of the forces by individual collisions over a period of time. In most times, the net force is nearly zero because collisions take place on every direction. However, the number of collisions is finite and fluctuates from time to time. Therefore, the net force also fluctuates and occasionally it is big enough to change the velocity of the particle appreciatively.\relax }}{452}{figure.caption.167}%
\contentsline {figure}{\numberline {18.2\relax }{\ignorespaces Diffusion of the one-dimensional Brownian motion modeled by the overdamped Langevin equation. Parameter values are $T=1$, $\gamma =0.1$, and thus $D=10$.\relax }}{457}{figure.caption.168}%
\contentsline {figure}{\numberline {18.3\relax }{\ignorespaces Diffusion of the two-dimensional Brownian motion modeled by the overdamped Langevin equations. Parameter values are $T=1$, $\gamma =0.1$, and thus $D=10$.\relax }}{457}{figure.caption.169}%
\contentsline {figure}{\numberline {18.4\relax }{\ignorespaces Ornstein-Uhlenbeck process is simulated with the langevin equation. The parameter values $M=1$ and $\gamma =0.1$ is used. The theoretical correlation time is $\tau _\text {c}=10$.\relax }}{460}{figure.caption.170}%
\contentsline {figure}{\numberline {18.5\relax }{\ignorespaces Brownian harmonic oscillator. Parameter values $M=1$, $\gamma =0.1$, and $k=1$ are used. The Overdumpled Langevin equation (\ref {eq:ou_x}) is integrated with $\Delta t=0.005$.\relax }}{460}{figure.caption.171}%
\contentsline {figure}{\numberline {18.6\relax }{\ignorespaces Mechanism of a flashing ratchet. When the potential is on, the particles are localized near the bottom of the potential. As soon as the potential is turned off, the particles diffuse. If there is an external force, they also drift (to the left in this setting). By the time when the potential is turned on again, the distribution is wide enough to reach adjacent potential minima. However, due to the asymmetry in the potential, the chance that the particles go to the right is higher against the external force. In this model the Brownian particles rectified the thermal fluctuation and move to the right on average.\relax }}{461}{figure.caption.172}%
\contentsline {figure}{\numberline {18.7\relax }{\ignorespaces Langevin simulation of the flashing ratchet. The inset in the left panel shows the total potential [Eq. (\ref {eq:ratchet_potential})]. The solid line indicates the potential due to the external force to the right. Parameter values $k_\text {B}T=1$, $\gamma =0.1$, $L=1$, $U_0=10$, and $F_\text {ext}=2$ are used. The corresponding diffusion constant is $D=10$. The potential is alternatively on for $\tau _\text {on}=10$ and off for $\tau _\text {off}=10$. The left panel shows that the particles are moving to the left despite that the external force is applied to the right. The mean position shown in the right panel indicates that the particles move to the left with constant velocity on average. \relax }}{463}{figure.caption.173}%
\contentsline {figure}{\numberline {18.8\relax }{\ignorespaces Bistable potentials used in the stochastic resonance model.\relax }}{463}{figure.caption.174}%
\contentsline {figure}{\numberline {18.9\relax }{\ignorespaces Stochastic Resonance. The upper panels show the trajectories of the Brownian particle and the lower panels plot the power spectrum of the corresponding trajectories. Parameter values $U_0=10, A=1, \Omega =0.5$ are fixed. There different noise intensity, $D=0.5$ (left), $D=1.2$ (center) and $D=2.0$ (right) are used. The trajectory in the center panel shows that the Brownian particle roughly flows the input signal. The power spectrum clearly shos a peak at $\omega =\Omega $, indicating that the input and output signals are in resonance.\relax }}{465}{figure.caption.175}%
\gdef \the@ipfilectr {}
\gdef \the@ipfilectr {@-19}
\contentsline {figure}{\numberline {19.1\relax }{\ignorespaces Local minimization along the downhill goes down to the local minimum. The global optimization must find the global minimum and thus the search method must be able to go over the barriers between minimums.\relax }}{486}{figure.caption.179}%
\contentsline {figure}{\numberline {19.2\relax }{\ignorespaces The particles can jump between two minimums over the barrier. The transition rate $k_\rightarrow .$ from the deeper minimum at $x_1$ to the shallow minimum at $x_2$ is smaller than the other direction of transition rate $k_\leftarrow $. Its ratio is given by $k_\rightarrow /k_\leftarrow = \text {e}^{-\Delta F/T}$. The particle current is $N_1 k_\rightarrow - N_2 k_\leftarrow $ where $N_i$ is the number of particle in the basin of $i$-th minimum. At thermal equilibrium particle current must vanish (detailed balance). Therefore, $N_1/N_2 = k_\leftarrow /k_\rightarrow > 1$. At thermal equilibrium, more particles are found in the deeper minimum than the other.\relax }}{488}{figure.caption.180}%
\contentsline {figure}{\numberline {19.3\relax }{\ignorespaces Searching a global minimum by the simulated annealing method. Eight samplers explore the landscape of the fitness function based on Metropolis method. (a) The fitness landscape shows several local minimums and the rather high barrier between minimums. At the end of the annealing three out of eight samplers are trapped in the basin of the global minimum. The remaining samplers are trapped in local minimums. (b) The evolution of the temperature indicates the exponential cooling schedule. The lowest fitness values among the population randomly changes when the temperature is high. As temperature is reduced, the samples are trapped in basins of minimums and the lowest fitness value no longer changes significantly.\relax }}{490}{figure.caption.181}%
\contentsline {figure}{\numberline {19.4\relax }{\ignorespaces Schematic diagram of genetic algorithm. Evolution of a species which has only one gene. The population consists of four individuals. The horizontal axis indicates \textit {genotype}. The genes of the first generation are chosen at random. A half of the population die due to the high fitness values. The remaining two individuals become a bleeding pair and generate two children whose genotype is between parent's gene. Now the size of population is back to four. The same process is repeated. The third and forth generations are shown. The unfit individuals die and the survived individuals are localized near the global minimum.\relax }}{491}{figure.caption.182}%
\contentsline {figure}{\numberline {19.5\relax }{\ignorespaces Effects of mutation: The left panel shows that all four individuals are trapped in the basin of a local minimum. Their children will be also in the same basin. To avoid this situation, an individual jumps to a random location (pink). This is the mutation. After the mutation, the individuals with higher fitness values die. If the mutated individual happened to fit better than others, it survives. The children born from the mutant are now outside of the basin of the local minimum. Now the four individuals are spread over three different minimums and thus the diversity of the population increased by the mutation.\relax }}{491}{figure.caption.183}%
\contentsline {figure}{\numberline {19.6\relax }{\ignorespaces Knuth shuffle algorithm\relax }}{493}{figure.caption.184}%
\contentsline {figure}{\numberline {19.7\relax }{\ignorespaces The evolution of the best fitness value. The best gene of the population gradually improves as the generation moves on. After 20th generation, the best fitness stays almost constant, indicating that the global minimum is discovered.\relax }}{495}{figure.caption.185}%
\contentsline {figure}{\numberline {19.8\relax }{\ignorespaces The noisy data (Table \ref {tbl:Gaussian}) is plotted with red circles with the error bars. The solid line is the result of the optimization using the genetic algorithm.\relax }}{496}{figure.caption.187}%
\contentsline {figure}{\numberline {19.9\relax }{\ignorespaces Thomson problem: Place $N$ point charges on the surface of a sphere such that the electrostatic potential energy is at the global minimum.\relax }}{497}{figure.caption.188}%
\gdef \the@ipfilectr {}
